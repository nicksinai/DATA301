{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:** learn to fit models to data using linear and non-linear regression.\n",
    "\n",
    "This material was developed by Brian Granger, adapted from material from Jake VanderPlas and Jennifer Klay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Data Science it is common to start with data and develop a *model* of that data. Such models can help to explain the data and make predictions about future observations. In fields like Physics, these models are often given in the form of differential equations, whose solutions explain and predict the data. In most other fields, such differential equations are not known. Often, models have to include sources of uncertainty and randomness. Given a set of data, *fitting* a model to the data is the process of tuning the parameters of the model to *best* explain the data.\n",
    "\n",
    "When a model has a linear dependence on its parameters, such as $a x^2 + b x + c$, this process is known as *linear regression*.  When a model has a non-linear dependence on its parameters, such as $ a e^{bx} $, this process in known as non-linear regression. Thus, fitting data to a straight line model of $m x + b $ is linear regression, because of its linear dependence on $m$ and $b$ (rather than $x$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a straight line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classical example of fitting a model is finding the slope and intercept of a straight line that goes through a set of data points $\\{x_i,y_i\\}$. For a straight line the model is:\n",
    "\n",
    "$$\n",
    "y_{model}(x) = mx + b\n",
    "$$\n",
    "\n",
    "Given this model, we can define a metric, or *cost function*, that quantifies the error the model makes. One commonly used metric is the [mean-squared error]() or MSE, which depends on the deviation of the model from each data point ($y_i - y_{model}(x_i)$).\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{N-2} \\sum_{i=1}^N \\left( y_i - y_{model}(x) \\right)^2\n",
    "$$\n",
    "\n",
    "When the MSE is small, the model's predictions will be close the data points. Likewise, when the MSE is large, the model's predictions will be far from the data points. Given this, our task is to minimize the MSE with respect to the model parameters $\\theta = [m, b]$ in order to find the best fit.\n",
    "\n",
    "The factor $N-2$ is called the *degrees of freedom* and is equal to the number of data points minus the number of model parameters.\n",
    "\n",
    "To illustrate linear regression, let's create a synthetic data set with a known slope and intercept, but random noise that is additive and normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 50\n",
    "m_true = 2\n",
    "b_true = -1\n",
    "dy = 2.0 # uncertainty of each point\n",
    "\n",
    "np.random.seed(0)\n",
    "xdata = 10 * np.random.random(N) # don't use regularly spaced data\n",
    "ydata = b_true + m_true * xdata + np.random.normal(0.0, dy, size=N) # our errors are additive\n",
    "\n",
    "plt.scatter(xdata, ydata)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to see visually how changing the model parameters changes the MSE. By using IPython's `interact` function, we can create a user interface that allows us to pick a slope and intercept interactively and see the resulting line and $\\chi^2$ value.\n",
    "\n",
    "Here is the function we want to minimize. Note how we have combined the two parameters into a single parameters vector $\\theta = [m, b]$, which is the first argument of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(theta, x, y):\n",
    "    b = theta[0]\n",
    "    m = theta[1]\n",
    "    return np.sum((y - b - m*x) ** 2)/(len(x)-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def manual_fit(b, m):\n",
    "    modely = m*xdata + b\n",
    "    plt.plot(xdata, modely)\n",
    "    plt.scatter(xdata, ydata)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim(-2,12)\n",
    "    plt.ylim(-5,20)\n",
    "    plt.text(1, 15, 'b={0:.2f}'.format(b))\n",
    "    plt.text(1, 12.5, 'm={0:.2f}'.format(m))\n",
    "    plt.text(1, 10.0, '$MSE$={0:.2f}'.format(mse([b,m],xdata,ydata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interact(manual_fit, b=(-3.0,3.0,0.01), m=(0.0,4.0,0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and play with the sliders and try to:\n",
    "\n",
    "* Find the lowest value of the MSE.\n",
    "* Find the \"best\" line through the data points.\n",
    "\n",
    "You should see that these two conditions coincide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize the MSE using `scipy.optimize.minimize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how minimizing the MSE gives the best parameters in a model, let's perform this minimization numerically using `scipy.optimize.minimize`. We have already defined the function we want to minimize, `mse`, so we only have to pass it to `minimize` along with an initial guess and the additional arguments (the raw data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_guess = [0.0,1.0]\n",
    "result = opt.minimize(mse, theta_guess, args=(xdata,ydata))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the values of $b$ and $m$ that minimize $\\chi^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_best = result.x\n",
    "print(theta_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are close to the true values of $b=-1$ and $m=2$. The reason our values are different is that our data set has a limited number of points. In general, we expect that as the number of points in our data set increases, the model parameters will converge to the true values. But having a limited number of data points is not a problem - it is a reality of most data collection processes.\n",
    "\n",
    "We can plot the raw data and the best fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(0,10.0)\n",
    "yfit = theta_best[1]*xfit + theta_best[0]\n",
    "\n",
    "plt.plot(xfit, yfit)\n",
    "plt.scatter(xdata, ydata)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize the MSE using `scipy.optimize.leastsq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing regression by minimizing the MSE is known as *least squares* regression, because we are minimizing the sum of squares of the deviations. The linear version of this is known as *linear least squares*. For this case, SciPy provides a purpose built function, `scipy.optimize.leastsq`. Instead of taking the MSE function to minimize, `leastsq` takes a function that computes the deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deviations(theta, x, y):\n",
    "    return y - theta[0] - theta[1] * x\n",
    "\n",
    "result = opt.leastsq(deviations, theta_guess, args=(xdata, ydata), full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have passed the `full_output=True` option. When this is passed the [covariance matrix](http://en.wikipedia.org/wiki/Covariance_matrix) $\\Sigma_{ij}$ of the model parameters is also returned. The uncertainties (as standard deviations) in the parameters are the square roots of the diagonal elements of the covariance matrix:\n",
    "\n",
    "$$ \\sigma_i = \\sqrt{\\Sigma_{ii}} $$\n",
    "\n",
    "A proof of this is beyond the scope of the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_best = result[0]\n",
    "theta_cov = result[1]\n",
    "print('b = {0:.3f} +/- {1:.3f}'.format(theta_best[0], np.sqrt(theta_cov[0,0])))\n",
    "print('m = {0:.3f} +/- {1:.3f}'.format(theta_best[1], np.sqrt(theta_cov[1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again plot the raw data and best fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yfit = theta_best[0] + theta_best[1] * xfit\n",
    "\n",
    "plt.scatter(xdata, ydata)\n",
    "plt.plot(xfit, yfit)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting using `scipy.optimize.curve_fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy also provides a general curve fitting function, `curve_fit`, that can handle both linear and non-linear models. This function: \n",
    "\n",
    "* Allows you to directly specify the model as a function, rather than the cost function (it assumes the MSE).\n",
    "* Returns the covariance matrix for the parameters that provides estimates of the errors in each of the parameters.\n",
    "\n",
    "Let's apply `curve_fit` to the above data. First we define a model function. The first argument should be the independent variable of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x, b, m):\n",
    "    return m*x+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call `curve_fit` passing the model function and the raw data. The uncertainties of each data point are provided with the `sigma` keyword argument. If there are no uncertainties, this can be omitted. By default the uncertainties are treated as relative. To treat them as absolute, pass the `absolute_sigma=True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_best, theta_cov = opt.curve_fit(model, xdata, ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, display the optimal values of $b$ and $m$ along with their uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('b = {0:.3f} +/- {1:.3f}'.format(theta_best[0], np.sqrt(theta_cov[0,0])))\n",
    "print('m = {0:.3f} +/- {1:.3f}'.format(theta_best[1], np.sqrt(theta_cov[1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again plot the raw data and best fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(0,10.0)\n",
    "yfit = theta_best[1]*xfit + theta_best[0]\n",
    "\n",
    "plt.plot(xfit, yfit)\n",
    "plt.scatter(xdata, ydata)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been using a linear model $y_{model}(x) = m x +b$. Remember this model was linear, not because of its dependence on $x$, but on $b$ and $m$. A non-linear model will have a non-linear dependece on the model parameters. Examples are $A e^{B x}$, $A \\cos{B x}$, etc. In this section we will generate data for the following non-linear model:\n",
    "\n",
    "$$y_{model}(x) = Ae^{Bx}$$\n",
    "\n",
    "and fit that data using `curve_fit`. Let's start out by using this model to generate a data set to use for our fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npoints = 20\n",
    "Atrue = 10.0\n",
    "Btrue = -0.2\n",
    "tdata = np.linspace(0.0, 20.0, npoints)\n",
    "dy = np.random.normal(0.0, 0.1, size=npoints)\n",
    "ytdata = Atrue*np.exp(Btrue*tdata) + dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(tdata, ytdata, 'k.')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y(t)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can use non-linear regression to recover the true values of our model parameters. First define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_model(x, A, B):\n",
    "    return A*np.exp(x*B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use `curve_fit` to fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_best, theta_cov = opt.curve_fit(exp_model, tdata, ytdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our optimized parameters are close to the true values of $A=10$ and $B=-0.2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('A = {0:.3f} +/- {1:.3f}'.format(theta_best[0], np.sqrt(theta_cov[0,0])))\n",
    "print('B = {0:.3f} +/- {1:.3f}'.format(theta_best[1], np.sqrt(theta_cov[1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the raw data and fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(0,20)\n",
    "yfit = exp_model(xfit, theta_best[0], theta_best[1])\n",
    "plt.plot(xfit, yfit)\n",
    "plt.plot(tdata, ytdata, 'k.')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y(t)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about transforming to a linear model\n",
    "\n",
    "Another approach to dealing with non-linear models is to linearize them with a transformation. For example, the exponential model used above,\n",
    "\n",
    "$$y_{model}(x) = Ae^{Bx},$$\n",
    "\n",
    "can be linearized by taking the natural log of both sides:\n",
    "\n",
    "$$ ln(y) = ln(A) + B x $$\n",
    "\n",
    "This model is linear in the parameters $ln(A)$ and $B$ and can be treated as a standard linear regression problem. This approach is used in most introductory physics laboratories. **However, in most cases, transforming to a linear model will give a poor fit. The reasons for this are a bit subtle, but here is the basic idea:\n",
    "\n",
    "* Least squares regression assumes that errors are symmetric, additive and normally distributed. This assumption has been present throughout this notebook, when we generated data by *adding* a small amount of randomness to our data using `np.random.normal`.\n",
    "* Transforming the data with a non-linear transformation, such as the square root, exponential or logarithm will not lead to errors that follow this assumption.\n",
    "* However, in the rare case that there are no (minimal) random errors in the original data set, the transformation approach will give the same result as the non-linear regression on the original model.\n",
    "\n",
    "Here is a [nice discussion](http://www.mathworks.com/help/stats/examples/pitfalls-in-fitting-nonlinear-models-by-transforming-to-linearity.html) of this in the Matlab documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of the examples in this notebook, we started with a model and used that model to generate data. This was done to make it easy to check the predicted model parameters with the true values used to create the data set. However, in the real world, you almost never know the model underlying the data. Because of this, there is an additional step called *model selection* where you have to figure out a way to pick a good model. This is a notoriously difficult problem, especially when the randomness in the data is large.\n",
    "\n",
    "* Pick the simplest possible model. In general picking a more complex model will give a better fit. However, it won't be a useful model and will make poor predictions about future data. This is known as [overfitting](http://en.wikipedia.org/wiki/Overfitting).\n",
    "* Whenever possible, pick a model that has a underlying theoretical foundation or motivation. For example, in Physics, most of our models come from well tested differential equations.\n",
    "* There are more advanced methods (AIC,BIC) that can assist in this model selection process. A good discussion can be found in [this notebook](https://github.com/jakevdp/2014_fall_ASTR599/blob/master/notebooks/14_Optimization.ipynb) by Jake VanderPlas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

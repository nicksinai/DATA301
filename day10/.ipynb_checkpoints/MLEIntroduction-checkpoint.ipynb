{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:** Understand maximum likelihood estimation and hwo to use it to infer parameters of probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have seen how to generate data from a probability distribution. Usually the underlying probability distribution depends on some parameters, such as $\\mu$ and $\\sigma$. For the purpose of this notebook, let's bundle up those parameters into a vector $\\theta = [\\theta_0, \\theta_1, \\ldots]$. The probability distribution for $x$, or probability density function, is usually denoted as:\n",
    "\n",
    "$$P(x \\mid \\theta)$$\n",
    "\n",
    "We expect this function to integrate to unity:\n",
    "\n",
    "$$ \\int P(x \\mid \\theta) dx = 1 $$\n",
    "\n",
    "Now we want to flip things around and imagine that instead of starting with the parameters $\\theta$ and generating data, we start with an array of data and want to find what value of $\\theta$ best describes the underlying probability distribution. This is called *statistical inference*.\n",
    "\n",
    "One approach for performing *inference* is called [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood). The foundation of this method is to interpret the above probability distribution in slightly different manner. Instead, we introduce the likelihood of a single data point $x$ as:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta \\mid x) = P(x \\mid \\theta) $$\n",
    "\n",
    "This is the *likelihood* of $\\theta$ given our data $x$. For multiple, independent samples $[x_0, x_1, \\ldots]$ the likelihood is simply the product:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta \\mid x_0, x_1, \\ldots) = \\prod_i \\mathcal{L}(\\theta \\mid x_i) = \\prod_i P(x_i \\mid \\theta) $$\n",
    "\n",
    "The maximum likelihood method consists of finding the value of $\\theta$ that maximizes this likelihood $ \\mathcal{L}(\\theta \\mid x_0, x_1, \\ldots) $. It is often more natural to maximize the natural log of this function, or the *log-likelihood*:\n",
    "\n",
    "$$ ln \\mathcal{L}(\\theta \\mid x_0, x_1, \\ldots) $$\n",
    "\n",
    "Or, we can throw in a minus sign and *minimize the negative log-likelihood*:\n",
    "\n",
    "$$ -ln \\mathcal{L}(\\theta \\mid x_0, x_1, \\ldots) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE for the normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how MLE works for the normal distribution. The probability density function is:\n",
    "\n",
    "$$ P(x \\mid \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\left[-\\frac{(x-\\mu)^2}{2 \\sigma^2}\\right]}$$\n",
    "\n",
    "The log-liklihood for a single sample is then:\n",
    "\n",
    "$$ ln \\mathcal{L}(\\mu,\\sigma^2 \\mid x) =  - \\frac{1}{2} ln(2 \\pi \\sigma^2) - \\frac{(x-\\mu)^2}{2 \\sigma^2} $$\n",
    "\n",
    "For an array of values $[x_1, x_2, \\ldots]$ the negative log-likelihood is then:\n",
    "\n",
    "$$ -ln \\mathcal{L}(\\mu,\\sigma^2 \\mid x_1, x_2, \\ldots) = \\frac{n}{2} ln(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2 $$\n",
    "\n",
    "This is the function we need to minimize to find our estimates $\\hat{\\mu}$ and $\\hat{\\sigma}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = 5.0\n",
    "sigma2 = 4.0\n",
    "data = np.random.normal(5.0, np.sqrt(sigma2), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our distribution of generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(data)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Generated Sample Distribution for the Normal Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_log_llh(theta, data):\n",
    "    \"\"\"Return the negative log-likelihood for the normal distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: tuple\n",
    "        The parameters [mu, sigma**2] of the normal distribution.\n",
    "    data: ndarra\n",
    "        An array of data points that are being modelled by the normal distribution.\n",
    "    \"\"\"\n",
    "    mu = theta[0]\n",
    "    sigma2 = theta[1]\n",
    "    n = len(data)\n",
    "    result = 0.5*n*np.log(2.0*np.pi*sigma2) + (1.0/(2.0*sigma2))*np.sum((data-mu)**2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_llh([1.0,1.0], data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to minimize this by hand using `interact`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_log_llh(mu, sigma2):\n",
    "    print(log_llh((mu, sigma2), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interact(print_log_llh, mu=(-10.0, 10.0, 0.1), sigma2=(0.1, 10.0, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to confirm this result by minimizing the negative log-likelihood numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "fit = optimize.minimize(lambda theta: log_llh(theta, data), [2.0,2.0])\n",
    "fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated parameters are stored in the `x` attribute of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('mu:       ', fit.x[0])\n",
    "print('sigma**2: ', fit.x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of the normal distribution, these values closely match the sample mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('sample mu:      ', data.mean())\n",
    "print('sample sigma**2:', data.var())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

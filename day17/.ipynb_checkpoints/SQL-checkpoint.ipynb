{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives**: Learn how to connect to and query SQL databases and get the results back as Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SQL](http://en.wikipedia.org/wiki/SQL), or *Structured Query Lanauge* is a programming language focused on read, writing and transforming table based data sets, called *Relational Databases*. There are a number of reasons you might want to work with data in a SQL database:\n",
    "\n",
    "* They offer high performance for a limited set of operations. They are much faster than Pandas for many types of operations, but are more limited.\n",
    "* They can handle data sets that don't fit into memory. Pandas DataFrames have to fit in the memory of your computer, which limits their size. Data bases are typically stored on disk and don't have that limitation. This opens the door for working with much larger data sets. Most \"big data\" tools offer some sort of SQL interfaces.\n",
    "* The data you want/need to work with may already be in a SQL database.\n",
    "\n",
    "\n",
    "There are numerous free and open source databases that offer SQL interfaces:\n",
    "\n",
    "* [SQLite](http://www.sqlite.org/)\n",
    "* [MySQL](http://www.mysql.com/)\n",
    "* [PostgreSQL](http://www.postgresql.org/)\n",
    "\n",
    "It is a good idea to start with SQLite as it has good performance characteristics, doesn't require any specialized setup and comes already installed with Python.\n",
    "\n",
    "There are a number of different Python packages for working with SQL databases:\n",
    "\n",
    "* [SqlAlchemy](http://www.sqlalchemy.org/): foundation layer that other tools rely on.\n",
    "* [pandas.io.sql](http://pandas.pydata.org/pandas-docs/stable/io.html#io-sql)\n",
    "* [Blaze](http://blaze.pydata.org/docs/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in working with an existing SQL database is to connect to it. This involves figuring out the appropriate connection string and passing it to `sqlalchemy.create_engine`. Here is the logic for our SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘titanic.db’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm titanic.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "te = create_engine('sqlite:////datitanic.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, this will use the file `titanic.db` in the current directory, which doesn't yet exist. SqlAlchemy will create it when needed. Here is the SqlAlchemy documentation for [engines and connection strings](http://docs.sqlalchemy.org/en/rel_0_9/core/engines.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pandas.io.sql`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has basic capabilties to reading, writing and querying SQL databases. These function are in the `pandas.io.sql` subpackage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.io import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write a table to a SQL database using the `to_sql` method, which takes the name of the table and an engine object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdf.to_sql('people', te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls *.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a populated database table, we can use Pandas to query the data set. The first thing we might want to do is read the entire table back into Python as a `DataFrame`. To emphasize that this is a new object, let's delete the original `DataFrame` we had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the `read_sql_table` function with the table name and engine object to get the entire table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tdf = sql.read_sql_table('people', te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: don't do this unless you know exactly how large the table is. Databases can contain tables that are many times larger than the available memory on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real power of SQL databases comes from being able to query them using the SQL language. This can be done in Pandas using the `read_sql_query` function. To use this function, we need to cover the basic syntax of the SQL language. Here is a [high-level overview](http://www.w3schools.com/sql/default.asp) of the SQL language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SELECT` statement is the most important part of the SQL language and is used to reads rows from a database table or tables. Here is an example that reads all (`*`) rows from the `people` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql.read_sql_query('SELECT * FROM people;', te).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read particular columns by name as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql.read_sql_query('SELECT survived, sex, age, fare FROM people;', te).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `LIMIT` clause to limit the number of rows that are returned. This is much more efficient that reading all  rows and then using `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql.read_sql_query(\"\"\"\n",
    "SELECT survived, sex, age, fare\n",
    "  FROM people\n",
    " LIMIT 10;\n",
    "\"\"\", te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `WHERE` clause to pick a subset of rows that satisfy certain criteria or tests. Notice how the notation for these tests is sligthly different from Python and Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql.read_sql_query(\"\"\"\n",
    "SELECT survived, sex, age, fare\n",
    "  FROM people\n",
    " WHERE age>20 AND survived=0 and sex=\"male\";\n",
    "\"\"\", te).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORDER BY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sort the resulting rows using the `ORDER BY` clause and the name of a column or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql.read_sql_query(\"\"\"\n",
    "SELECT survived, sex, age, fare\n",
    "  FROM people\n",
    " WHERE age>20 AND survived=0 and sex=\"male\"\n",
    " ORDER BY fare;\n",
    "\"\"\", te).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `DESC` to reverse the order of the sort to descending:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql.read_sql_query(\"\"\"\n",
    "SELECT survived, sex, age, fare\n",
    "  FROM people\n",
    " WHERE age>20 AND survived=0 and sex=\"male\"\n",
    " ORDER BY fare, age DESC;\n",
    "\"\"\", te).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL offers a host of functions that can applied to columns of data:\n",
    "\n",
    "* `AVG()`\n",
    "* `COUNT()`\n",
    "* `MIN()`\n",
    "* `MAX()`\n",
    "* `SUM()`\n",
    "\n",
    "Here we compute the number of males and the average fare they paid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql.read_sql_query(\"\"\"\n",
    "SELECT AVG(fare), COUNT(sex)\n",
    "  FROM people\n",
    " WHERE sex=\"male\";\n",
    "\"\"\", te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = new_tdf[new_tdf.sex=='male']\n",
    "pd.DataFrame({'COUNT(sex)': [filtered.sex.count()], 'AVG(fare)': [filtered.fare.mean()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP BY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GROUP BY` clause allows you to perform split-apply-combine to a table to perform a wide range of transformations and aggregations. This works almost identically to the `groupby` method in Pandas. Here we using `GROUP BY` to compute averarge fares by gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql.read_sql_query(\"\"\"\n",
    "SELECT sex, AVG(fare)\n",
    "  FROM people\n",
    " GROUP BY sex\n",
    "\"\"\", te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same computation on a Pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_tdf.groupby('sex')['fare'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database introspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important things to do when you start exploring a new database is to survey what tables the database has and what columns those tables have. This is simple with `sqlalchemy.inspect` and Pandas.\n",
    "\n",
    "For this part, we will use the [Chinook Database](https://chinookdatabase.codeplex.com/) of music:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ce = create_engine('sqlite:///data/Chinook_Sqlite.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inspector for that database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "inspector = inspect(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the column names from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inspector.get_table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the columns from a particular table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inspector.get_columns('Artist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple function for retrieving the the columns names and types as a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def table_info(table_name, i):\n",
    "    \"\"\"Shwo the column names and types as a Pandas DataFrame.\"\"\"\n",
    "    return pd.DataFrame(i.get_columns(table_name))[['name','type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Album` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_info('Album', inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Artist` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_info('Artist', inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how both the `Artist` and `Album` table have an `ArtistId` column. Let's perform a join on those two tables. Similar to Pandas, SQL support 4 types of joins (INNER, LEFT, RIGHT, FULL). Notice how we are using AS to rename columns in the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql.read_sql_query(\"\"\"\n",
    "SELECT Album.Title AS AlbumTitle, Artist.Name AS Artist FROM Album\n",
    "    INNER JOIN Artist\n",
    "    ON Artist.ArtistId=Album.ArtistId\n",
    "    ORDER BY Artist.Name;\n",
    "\"\"\", ce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
